{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Just add the zip file path properly. To run the code."
      ],
      "metadata": {
        "id": "VDyqzxM-68di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3D CNN"
      ],
      "metadata": {
        "id": "Z5YMkXX16lWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "aoTYXLo5HXWM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPgEEPlP2KsD",
        "outputId": "65314693-b994-4edd-b632-8f04c0859136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/checkpoints.zip\n",
            "   creating: checkpoints/my_3d_cnn_model/\n",
            " extracting: checkpoints/my_3d_cnn_model/fingerprint.pb  \n",
            "  inflating: checkpoints/my_3d_cnn_model/keras_metadata.pb  \n",
            "  inflating: checkpoints/my_3d_cnn_model/saved_model.pb  \n",
            "   creating: checkpoints/my_3d_cnn_model/variables/\n",
            "  inflating: checkpoints/my_3d_cnn_model/variables/variables.data-00000-of-00001  \n",
            "  inflating: checkpoints/my_3d_cnn_model/variables/variables.index  \n",
            "  inflating: checkpoints/my_vit_dense_model.h5  \n",
            "  inflating: checkpoints/my_vit_lstm_model.h5  \n",
            "Archive:  /content/Cardiac-Video-Sequence.zip\n",
            "   creating: Cardiac-Video-Sequence/\n",
            "  inflating: __MACOSX/._Cardiac-Video-Sequence  \n",
            "  inflating: Cardiac-Video-Sequence/2023-11-15-cine-myo-masks-and-TOS.npy  \n",
            "  inflating: __MACOSX/Cardiac-Video-Sequence/._2023-11-15-cine-myo-masks-and-TOS.npy  \n",
            "  inflating: Cardiac-Video-Sequence/README.txt  \n",
            "  inflating: __MACOSX/Cardiac-Video-Sequence/._README.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/Cardiac-Video-Sequence.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from numpy import pad"
      ],
      "metadata": {
        "id": "wRfmYnoT5LpT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing  "
      ],
      "metadata": {
        "id": "M7bsOuf3Hb6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.load('/content/Cardiac-Video-Sequence/2023-11-15-cine-myo-masks-and-TOS.npy', allow_pickle=True)\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "time_stamps = []\n",
        "\n",
        "for i in range(128):\n",
        "    mask_volume.append(data[i]['cine_lv_myo_masks_cropped'])   # Get the myocardium mask of slice 0. It should be a (H, W, n_frames) volume\n",
        "    TOS_volume.append(data[i]['TOS'])  # Get the TOS curve of slice 0. It should be a (126, n_frames) 2D array\n",
        "\n",
        "print('Data:', mask_volume[0].shape)\n",
        "print('Ground Truth', TOS_volume[0].shape)\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "\n",
        "# Extract data\n",
        "for i in range(len(data)):\n",
        "    mask_volume.append(data[i]['cine_lv_myo_masks_cropped'])  # Myocardium masks\n",
        "    TOS_volume.append(data[i]['TOS'])  # TOS curves\n",
        "\n",
        "# Determine the maximum dimensions\n",
        "max_height = max(array.shape[0] for array in mask_volume)\n",
        "max_width = max(array.shape[1] for array in mask_volume)\n",
        "max_frames = max(array.shape[2] for array in mask_volume)\n",
        "\n",
        "# Function to pad an array to the maximum size\n",
        "def pad_array(array, max_height, max_width, max_frames):\n",
        "    pad_height = max_height - array.shape[0]\n",
        "    pad_width = max_width - array.shape[1]\n",
        "    pad_frames = max_frames - array.shape[2]\n",
        "    return pad(array, ((0, pad_height), (0, pad_width), (0, pad_frames)), 'constant', constant_values=0)\n",
        "\n",
        "# Apply padding to each array\n",
        "mask_volume_padded = [pad_array(array, max_height, max_width, max_frames) for array in mask_volume]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "mask_volume = np.array(mask_volume_padded)\n",
        "TOS_volume = np.array(TOS_volume)\n",
        "\n",
        "# Normalizing the mask volumes\n",
        "mask_volume = mask_volume / 255.0\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "mask_volume_train, mask_volume_test, TOS_volume_train, TOS_volume_test = train_test_split(mask_volume, TOS_volume, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data Shape:\", mask_volume_train.shape)\n",
        "print(\"Testing Data Shape:\", mask_volume_test.shape)\n",
        "print(\"Training Ground Truth Shape:\", TOS_volume_train.shape)\n",
        "print(\"Testing Ground Truth Shape:\", TOS_volume_test.shape)\n",
        "\n",
        "\n",
        "# Convert training and testing labels to integer values\n",
        "TOS_volume_train_int = np.round(TOS_volume_train).astype(int)\n",
        "TOS_volume_test_int = np.round(TOS_volume_test).astype(int)\n",
        "\n",
        "print(\"Training Ground Truth Shape:\", TOS_volume_train_int.shape)\n",
        "print(\"Testing Ground Truth Shape:\", TOS_volume_test_int.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_va1jIU27-B",
        "outputId": "b39826fd-33c5-45ca-b7c0-d30c4477ac1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: (80, 80, 25)\n",
            "Ground Truth (126,)\n",
            "Training Data Shape: (102, 80, 80, 25)\n",
            "Testing Data Shape: (26, 80, 80, 25)\n",
            "Training Ground Truth Shape: (102, 126)\n",
            "Testing Ground Truth Shape: (26, 126)\n",
            "Training Ground Truth Shape: (102, 126)\n",
            "Testing Ground Truth Shape: (26, 126)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and check the model on the test dataset on 3D CNN"
      ],
      "metadata": {
        "id": "J46aPtP_HfGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('/content/checkpoints/my_3d_cnn_model')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, mean_absolute_error = model.evaluate(mask_volume_test, TOS_volume_test)\n",
        "\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Mean Absolute Error:\", mean_absolute_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpKDsKY_4hYk",
        "outputId": "3647727c-beeb-45e1-9952-a7e3aa7c61aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 253ms/step - loss: 198.5519 - mean_absolute_error: 8.4136\n",
            "Test Loss: 198.55194091796875\n",
            "Test Mean Absolute Error: 8.413644790649414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note we have reported the test scores MAE value from the main 3D_CNN.ipynb file in the report. ANd not this. MAE = 8.5879\n",
        "\n",
        "----\n",
        "----"
      ],
      "metadata": {
        "id": "OWvI5K-h6Sbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT + Dense"
      ],
      "metadata": {
        "id": "HwofmWFN6on7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "IvlTpLt3HrFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Cardiac-Video-Sequence.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKPYm6Y1_cQ2",
        "outputId": "468edccd-3a8e-4382-fd1e-969891cafaa0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Cardiac-Video-Sequence.zip\n",
            "   creating: Cardiac-Video-Sequence/\n",
            "  inflating: __MACOSX/._Cardiac-Video-Sequence  \n",
            "  inflating: Cardiac-Video-Sequence/2023-11-15-cine-myo-masks-and-TOS.npy  \n",
            "  inflating: __MACOSX/Cardiac-Video-Sequence/._2023-11-15-cine-myo-masks-and-TOS.npy  \n",
            "  inflating: Cardiac-Video-Sequence/README.txt  \n",
            "  inflating: __MACOSX/Cardiac-Video-Sequence/._README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import pad\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "WT5nXZcy8uV9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing  "
      ],
      "metadata": {
        "id": "7RrMYqAUHzGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load('/content/Cardiac-Video-Sequence/2023-11-15-cine-myo-masks-and-TOS.npy', allow_pickle=True)\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "time_stamps = []\n",
        "\n",
        "for i in range(128):\n",
        "    mask_volume.append(data[i]['cine_lv_myo_masks_cropped'])   # Get the myocardium mask of slice 0. It should be a (H, W, n_frames) volume\n",
        "    TOS_volume.append(data[i]['TOS'])  # Get the TOS curve of slice 0. It should be a (126, n_frames) 2D array\n",
        "\n",
        "print('Data:', mask_volume[0].shape)\n",
        "print('Ground Truth', TOS_volume[0].shape)\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "\n",
        "for i in range(128):\n",
        "    mask = data[i]['cine_lv_myo_masks_cropped']   # Shape: (H, W, n_frames)\n",
        "    TOS = data[i]['TOS']                          # Shape: (126,)\n",
        "\n",
        "    # Example preprocessing step: averaging across the time dimension\n",
        "    mask_avg = np.mean(mask, axis=2)              # Shape: (H, W)\n",
        "\n",
        "    mask_volume.append(mask_avg)\n",
        "    TOS_volume.append(TOS)\n",
        "\n",
        "# Normalize the data\n",
        "mask_volume = np.array(mask_volume)\n",
        "mask_volume = (mask_volume - np.min(mask_volume)) / (np.max(mask_volume) - np.min(mask_volume))\n",
        "\n",
        "# Convert TOS_volume to a numpy array for consistency\n",
        "TOS_volume = np.array(TOS_volume)\n",
        "\n",
        "# Further steps include splitting the data into training, validation, and test sets,\n",
        "# and then batching the data for training.\n",
        "\n",
        "print('Data:', mask_volume.shape)\n",
        "print('Ground Truth', TOS_volume.shape)\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "\n",
        "# Extract data\n",
        "for i in range(len(data)):\n",
        "    mask_volume.append(data[i]['cine_lv_myo_masks_cropped'])  # Myocardium masks\n",
        "    TOS_volume.append(data[i]['TOS'])  # TOS curves\n",
        "\n",
        "# Determine the maximum dimensions\n",
        "max_height = max(array.shape[0] for array in mask_volume)\n",
        "max_width = max(array.shape[1] for array in mask_volume)\n",
        "max_frames = max(array.shape[2] for array in mask_volume)\n",
        "\n",
        "# Function to pad an array to the maximum size\n",
        "def pad_array(array, max_height, max_width, max_frames):\n",
        "    pad_height = max_height - array.shape[0]\n",
        "    pad_width = max_width - array.shape[1]\n",
        "    pad_frames = max_frames - array.shape[2]\n",
        "    return pad(array, ((0, pad_height), (0, pad_width), (0, pad_frames)), 'constant', constant_values=0)\n",
        "\n",
        "# Apply padding to each array\n",
        "mask_volume_padded = [pad_array(array, max_height, max_width, max_frames) for array in mask_volume]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "mask_volume = np.array(mask_volume_padded)\n",
        "TOS_volume = np.array(TOS_volume)\n",
        "\n",
        "# Normalizing the mask volumes\n",
        "mask_volume = mask_volume / 255.0\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "mask_volume_train, mask_volume_test, TOS_volume_train, TOS_volume_test = train_test_split(mask_volume, TOS_volume, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data Shape:\", mask_volume_train.shape)\n",
        "print(\"Testing Data Shape:\", mask_volume_test.shape)\n",
        "print(\"Training Ground Truth Shape:\", TOS_volume_train.shape)\n",
        "print(\"Testing Ground Truth Shape:\", TOS_volume_test.shape)\n",
        "\n",
        "# Splitting data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(mask_volume, TOS_volume, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_val.shape,y_val.shape)\n",
        "print(X_test.shape,y_test.shape)\n",
        "\n",
        "# Convert training and testing labels to integer values\n",
        "TOS_volume_train_int = np.round(TOS_volume_train).astype(int)\n",
        "TOS_volume_test_int = np.round(TOS_volume_test).astype(int)\n",
        "\n",
        "print(\"Training Ground Truth Shape:\", TOS_volume_train_int.shape)\n",
        "print(\"Testing Ground Truth Shape:\", TOS_volume_test_int.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVU1tiGK5JGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af39f1b-0e1e-4a81-c9b0-2bc5012894a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: (80, 80, 25)\n",
            "Ground Truth (126,)\n",
            "Data: (128, 80, 80)\n",
            "Ground Truth (128, 126)\n",
            "Training Data Shape: (102, 80, 80, 25)\n",
            "Testing Data Shape: (26, 80, 80, 25)\n",
            "Training Ground Truth Shape: (102, 126)\n",
            "Testing Ground Truth Shape: (26, 126)\n",
            "(102, 80, 80, 25) (102, 126)\n",
            "(13, 80, 80, 25) (13, 126)\n",
            "(13, 80, 80, 25) (13, 126)\n",
            "Training Ground Truth Shape: (102, 126)\n",
            "Testing Ground Truth Shape: (26, 126)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and check the model on the test dataset on ViT+ Dense"
      ],
      "metadata": {
        "id": "-YUMKAeOHmGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model(\n",
        "    '/content/checkpoints/my_vit_dense_model.h5',\n",
        "    custom_objects={'KerasLayer': hub.KerasLayer}\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mean_absolute_error = loaded_model.evaluate(mask_volume_test, TOS_volume_test_int)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Mean Absolute Error:\", mean_absolute_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7EMex2O-ByP",
        "outputId": "54adb8bd-4582-418e-d7f7-26cf8ac68b15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 19s 19s/step - loss: 200.8010 - mean_absolute_error: 8.2325\n",
            "Test Loss: 200.80096435546875\n",
            "Test Mean Absolute Error: 8.232523918151855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "NSkReB28IGeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT + LSTM"
      ],
      "metadata": {
        "id": "TD05op2h6rMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "peisbX5HHs1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Cardiac-Video-Sequence.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txx9suIGGxQx",
        "outputId": "ce595ef6-efcd-40de-8475-5238635c20d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Cardiac-Video-Sequence.zip\n",
            "replace __MACOSX/._Cardiac-Video-Sequence? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._Cardiac-Video-Sequence  \n",
            "replace Cardiac-Video-Sequence/2023-11-15-cine-myo-masks-and-TOS.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: Cardiac-Video-Sequence/2023-11-15-cine-myo-masks-and-TOS.npy  \n",
            "  inflating: __MACOSX/Cardiac-Video-Sequence/._2023-11-15-cine-myo-masks-and-TOS.npy  \n",
            "  inflating: Cardiac-Video-Sequence/README.txt  \n",
            "  inflating: __MACOSX/Cardiac-Video-Sequence/._README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import pad\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "lv3wJeD2Gw2i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing  "
      ],
      "metadata": {
        "id": "rNK7Gfj0H0ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = np.load('/content/Cardiac-Video-Sequence/2023-11-15-cine-myo-masks-and-TOS.npy', allow_pickle=True)\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "time_stamps = []\n",
        "\n",
        "for i in range(128):\n",
        "    mask_volume.append(data[i]['cine_lv_myo_masks_cropped'])   # Get the myocardium mask of slice 0. It should be a (H, W, n_frames) volume\n",
        "    TOS_volume.append(data[i]['TOS'])  # Get the TOS curve of slice 0. It should be a (126, n_frames) 2D array\n",
        "\n",
        "print('Data:', mask_volume[0].shape)\n",
        "print('Ground Truth', TOS_volume[0].shape)\n",
        "\n",
        "\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "\n",
        "for i in range(128):\n",
        "    mask = data[i]['cine_lv_myo_masks_cropped']   # Shape: (H, W, n_frames)\n",
        "    TOS = data[i]['TOS']                          # Shape: (126,)\n",
        "\n",
        "    # Example preprocessing step: averaging across the time dimension\n",
        "    mask_avg = np.mean(mask, axis=2)              # Shape: (H, W)\n",
        "\n",
        "    mask_volume.append(mask_avg)\n",
        "    TOS_volume.append(TOS)\n",
        "\n",
        "# Normalize the data\n",
        "mask_volume = np.array(mask_volume)\n",
        "mask_volume = (mask_volume - np.min(mask_volume)) / (np.max(mask_volume) - np.min(mask_volume))\n",
        "\n",
        "# Convert TOS_volume to a numpy array for consistency\n",
        "TOS_volume = np.array(TOS_volume)\n",
        "\n",
        "# Further steps include splitting the data into training, validation, and test sets,\n",
        "# and then batching the data for training.\n",
        "\n",
        "print('Data:', mask_volume.shape)\n",
        "print('Ground Truth', TOS_volume.shape)\n",
        "\n",
        "mask_volume = []\n",
        "TOS_volume = []\n",
        "\n",
        "# Extract data\n",
        "for i in range(len(data)):\n",
        "    mask_volume.append(data[i]['cine_lv_myo_masks_cropped'])  # Myocardium masks\n",
        "    TOS_volume.append(data[i]['TOS'])  # TOS curves\n",
        "\n",
        "# Determine the maximum dimensions\n",
        "max_height = max(array.shape[0] for array in mask_volume)\n",
        "max_width = max(array.shape[1] for array in mask_volume)\n",
        "max_frames = max(array.shape[2] for array in mask_volume)\n",
        "\n",
        "# Function to pad an array to the maximum size\n",
        "def pad_array(array, max_height, max_width, max_frames):\n",
        "    pad_height = max_height - array.shape[0]\n",
        "    pad_width = max_width - array.shape[1]\n",
        "    pad_frames = max_frames - array.shape[2]\n",
        "    return pad(array, ((0, pad_height), (0, pad_width), (0, pad_frames)), 'constant', constant_values=0)\n",
        "\n",
        "# Apply padding to each array\n",
        "mask_volume_padded = [pad_array(array, max_height, max_width, max_frames) for array in mask_volume]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "mask_volume = np.array(mask_volume_padded)\n",
        "TOS_volume = np.array(TOS_volume)\n",
        "\n",
        "# Normalizing the mask volumes\n",
        "mask_volume = mask_volume / 255.0\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "mask_volume_train, mask_volume_test, TOS_volume_train, TOS_volume_test = train_test_split(mask_volume, TOS_volume, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data Shape:\", mask_volume_train.shape)\n",
        "print(\"Testing Data Shape:\", mask_volume_test.shape)\n",
        "print(\"Training Ground Truth Shape:\", TOS_volume_train.shape)\n",
        "print(\"Testing Ground Truth Shape:\", TOS_volume_test.shape)\n",
        "\n",
        "# Splitting data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(mask_volume, TOS_volume, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_val.shape,y_val.shape)\n",
        "print(X_test.shape,y_test.shape)\n",
        "\n",
        "# Convert training and testing labels to integer values\n",
        "TOS_volume_train_int = np.round(TOS_volume_train).astype(int)\n",
        "TOS_volume_test_int = np.round(TOS_volume_test).astype(int)\n",
        "\n",
        "print(\"Training Ground Truth Shape:\", TOS_volume_train_int.shape)\n",
        "print(\"Testing Ground Truth Shape:\", TOS_volume_test_int.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "S6E5Gfx86vDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37cd61c0-5e5b-4ff9-ed68-8553e34c2751"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: (80, 80, 25)\n",
            "Ground Truth (126,)\n",
            "Data: (128, 80, 80)\n",
            "Ground Truth (128, 126)\n",
            "Training Data Shape: (102, 80, 80, 25)\n",
            "Testing Data Shape: (26, 80, 80, 25)\n",
            "Training Ground Truth Shape: (102, 126)\n",
            "Testing Ground Truth Shape: (26, 126)\n",
            "(102, 80, 80, 25) (102, 126)\n",
            "(13, 80, 80, 25) (13, 126)\n",
            "(13, 80, 80, 25) (13, 126)\n",
            "Training Ground Truth Shape: (102, 126)\n",
            "Testing Ground Truth Shape: (26, 126)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and check the model on the test dataset on ViT+lstm"
      ],
      "metadata": {
        "id": "T1EwzH8YHnge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model(\n",
        "    '/content/checkpoints/my_vit_lstm_model.h5',\n",
        "    custom_objects={'KerasLayer': hub.KerasLayer}\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mean_absolute_error = loaded_model.evaluate(mask_volume_test, TOS_volume_test_int)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Mean Absolute Error:\", mean_absolute_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGzfcDAPGzg8",
        "outputId": "109eef08-dd9e-48cb-e360-02b33c5e7a10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 9s 9s/step - loss: 199.6137 - mean_absolute_error: 8.1138\n",
            "Test Loss: 199.61366271972656\n",
            "Test Mean Absolute Error: 8.113755226135254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "Thanks!\n",
        "\n",
        "Please note we have reported the test scores MAE value from the main Traning ipynb files\n",
        "\n",
        "Reach me out if you encounter any problem!\n",
        "rugvedm12@gmail.com\n",
        "😄\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "csvToYrWIIip"
      }
    }
  ]
}